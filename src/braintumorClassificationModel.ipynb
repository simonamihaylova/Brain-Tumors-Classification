{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "braintumorClassificationModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18RYZdfWlkVj"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX3Ptt7iHFko"
      },
      "source": [
        "## Importing the augmented dataset:\n",
        "*   `dataset_augmentation.ipynb` - Loads the original Brain Tumor Dataset (3064 T1-Weighted MRI images) and augments the dataset using techniques such as rotating, mirroring, flipping over an axis and salting.\n",
        "*   `augmented_images.npz` - Contains the full dataset after augmentation (15320 images and 15320 labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C8aFpWdfkmX",
        "outputId": "53bdcc97-425b-43e3-a0fc-27ddb69d1f2a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86WAVLHWfrAu"
      },
      "source": [
        "#images_path = '../dataset/augmented_images.npz'\n",
        "images_path = '/content/gdrive/MyDrive/augmented_images.npz'"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p8k914CgCt-",
        "outputId": "3570b8fc-504e-4993-cb2d-02353f4f73f2"
      },
      "source": [
        "with np.load(images_path) as data:\n",
        "    \n",
        "  images = data['images']\n",
        "  labels = data['labels']\n",
        "  print('images: ', images.shape)\n",
        "  print('labels:', labels.shape)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images:  (15320, 128, 128)\n",
            "labels: (15320,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNYgyzkuW13z"
      },
      "source": [
        "## Reformatting the data\n",
        "*   reformat into a tensorflow-friendly shape\n",
        "*   shuffle the data\n",
        "*   split the dataset into train, validation and test dataset with the following ratio: 80, 10, 10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU_fkUiOOJ9b",
        "outputId": "6dbf4c1a-2d19-4d06-9904-46f31a3dcd7f"
      },
      "source": [
        "num_labels = 3\n",
        "num_channels = 1 # MRI images are grayscale\n",
        "image_size = 128\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape(\n",
        "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "\n",
        "images, labels = reformat(images, labels)\n",
        "print('images:', images.shape)\n",
        "print('labels:', labels.shape)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images: (15320, 128, 128, 1)\n",
            "labels: (15320, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkgpcY5Z8JZ"
      },
      "source": [
        "# Shufling the two numpy arrays in unison\n",
        "from sklearn import utils\n",
        "images, labels = utils.shuffle(images,labels)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5xnI3jzYqMV",
        "outputId": "4b60f3ed-1103-44e9-966a-e527307df563"
      },
      "source": [
        "train_dataset, remaining_dataset, train_labels, remaining_labels = train_test_split(images, labels, train_size=0.8)\n",
        "valid_dataset,test_dataset, valid_labels, test_labels = train_test_split(remaining_dataset, remaining_labels, test_size=0.5)\n",
        "\n",
        "print('Training set:', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set:', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set:', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (12256, 128, 128, 1) (12256, 3)\n",
            "Validation set: (1532, 128, 128, 1) (1532, 3)\n",
            "Test set: (1532, 128, 128, 1) (1532, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB2DksU_sUF4",
        "outputId": "c6cd57f1-0337-4c3a-b8bd-487f3e6a0bc3"
      },
      "source": [
        "print(train_labels[0], train_labels[10416], train_labels[2010])"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0.] [1. 0. 0.] [1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5AnQ1tctZxC",
        "outputId": "8ebe2994-5b46-4c9c-b820-6f4cf20085d3"
      },
      "source": [
        "print(valid_labels[30], valid_labels[1531], valid_labels[200])"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1.] [0. 0. 1.] [0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbGysfpbq1I8"
      },
      "source": [
        "## Defining the brain tumor classification model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIRZ6rKSx_NC"
      },
      "source": [
        "batch_size = 32\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_dataset, train_labels)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKglS2kIkN3d"
      },
      "source": [
        "# Architecture:\n",
        "# Input: 128 x 128 x 1\n",
        "# Conv1: 128 x 128 x 64\n",
        "# MaxPool1: 64 x 64 x 64\n",
        "# Conv2: 64 x 64 x 128\n",
        "# MaxPool2: 32 x 32 x 128\n",
        "# Conv3: 32 x 32 x 256\n",
        "# MaxPool3: 16 x 16 x 256\n",
        "# FC: 16 * 16 * 256 , 256\n",
        "# Output: 256, 3\n",
        "\n",
        "# Architecture:\n",
        "# Input: 128 x 128 x 1\n",
        "# Conv1: 128 x 128 x 16\n",
        "# MaxPool1: 64 x 64 x 16\n",
        "# Conv2: 64 x 64 x 32\n",
        "# MaxPool2: 32 x 32 x 32\n",
        "# Conv3: 32 x 32 x 64\n",
        "# MaxPool3: 16 x 16 x 64\n",
        "# FC: 16 * 16 * 64 , 64\n",
        "# Output: 64, 3\n",
        "\n",
        "filter_size = 3\n",
        "depth_conv1 = 16\n",
        "depth_conv2 = 32\n",
        "depth_conv3 = 64\n",
        "\n",
        "weights = {\n",
        "    'wc1' : tf.Variable(tf.random.truncated_normal([filter_size, filter_size, num_channels, depth_conv1], stddev=0.1)),\n",
        "    'wc2' : tf.Variable(tf.random.truncated_normal([filter_size, filter_size, depth_conv1, depth_conv2], stddev=0.1)),\n",
        "    'wc3' : tf.Variable(tf.random.truncated_normal([filter_size, filter_size, depth_conv2, depth_conv3], stddev=0.1)),\n",
        "    'wfc' : tf.Variable(tf.random.truncated_normal([16 * 16 * depth_conv3, depth_conv3], stddev = 0.1)),\n",
        "    'wout': tf.Variable(tf.random.truncated_normal([depth_conv3, num_labels], stddev = 0.1)),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1' : tf.Variable(tf.constant(1.0, shape=[depth_conv1])),\n",
        "    'bc2' : tf.Variable(tf.constant(1.0, shape=[depth_conv2])),\n",
        "    'bc3' : tf.Variable(tf.constant(1.0, shape=[depth_conv3])),\n",
        "    'bfc' : tf.Variable(tf.constant(1.0, shape=[depth_conv3])),\n",
        "    'bout': tf.Variable(tf.constant(1.0, shape=[num_labels])),\n",
        "}\n"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMfZNXlb3qMO"
      },
      "source": [
        "# Wrapper functions for the convolutional and max pooling layers\n",
        "def conv2d(x, W, b, stride=1):\n",
        "  x = tf.nn.conv2d(x, W, strides=[1,stride,stride,1], padding='SAME')\n",
        "  x = tf.nn.bias_add(x, b)\n",
        "  return tf.nn.relu(x)\n",
        "\n",
        "def maxpool(x, k = 2):\n",
        "  return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,k,k,1], padding='SAME')\n"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ95OKGN62PL"
      },
      "source": [
        "def braintumor_classification_model(data):\n",
        "  conv_layer1 = conv2d(data, weights['wc1'], biases['bc1'])\n",
        "  conv_layer1 = maxpool(conv_layer1)\n",
        "\n",
        "  conv_layer2 = conv2d(conv_layer1, weights['wc2'], biases['bc2'])\n",
        "  conv_layer2 = maxpool(conv_layer2)\n",
        "\n",
        "  conv_layer3 = conv2d(conv_layer2, weights['wc3'], biases['bc3'])\n",
        "  conv_layer3 = maxpool(conv_layer3)\n",
        "\n",
        "  conv_layer3_shape = conv_layer3.get_shape().as_list()\n",
        "  fc_layer = tf.reshape(conv_layer3, [conv_layer3_shape[0], conv_layer3_shape[1] * conv_layer3_shape[2] * conv_layer3_shape[3]])\n",
        "  fc_layer = tf.add(tf.matmul(fc_layer, weights['wfc']), biases['bfc'])\n",
        "  fc_layer = tf.nn.relu(fc_layer)\n",
        "\n",
        "  output_layer = tf.add(tf.matmul(fc_layer, weights['wout']), biases['bout'])\n",
        "  return output_layer"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxtLhKGg-eGb"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29cMfrd-pNO"
      },
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkgJFuKQv8Pr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntS7puYcrgOy",
        "outputId": "fa6b75f0-d016-4ef1-f84c-5963adb863df"
      },
      "source": [
        "num_batches_val = 4 \n",
        "batch_size_val = int(len(valid_dataset) / num_batches) # 4 batches, 383 examples in each batch\n",
        "print(\"Num of examples in one batch - validation dataset:\", batch_size_val)\n",
        "valid_tf_dataset = tf.data.Dataset.from_tensor_slices((valid_dataset, valid_labels)).batch(batch_size_val)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of examples in one batch - validation dataset: 383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m0Sduke_LHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1145762a-5309-4c2f-f529-035bf16d079d"
      },
      "source": [
        "epochs = 2\n",
        "display_step = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (batch_x_train, batch_y_train) in enumerate(train_tf_dataset, 1):\n",
        "\n",
        "        with tf.GradientTape() as g:\n",
        "          logits = braintumor_classification_model(batch_x_train)\n",
        "          loss = compute_loss(batch_y_train, logits)\n",
        "\n",
        "        # Optimizer.\n",
        "        optimizer.minimize(loss, g.watched_variables(), tape=g)\n",
        "        \n",
        "        # Predictions for the training and validation data.\n",
        "        if step % display_step == 0:\n",
        "          train_prediction = tf.nn.softmax(logits)\n",
        "          train_acc = accuracy(train_prediction, batch_y_train)\n",
        "\n",
        "          valid_acc = 0\n",
        "\n",
        "          for (batch_x_val, batch_y_val) in valid_tf_dataset:  \n",
        "              valid_prediction = tf.nn.softmax(braintumor_classification_model(batch_x_val))\n",
        "              valid_acc += accuracy(valid_prediction, batch_y_val)\n",
        "          \n",
        "          print(\"step: %i, loss: %f, train acc: %f, validation acc: %f\" % (step, loss, train_acc, valid_acc / num_batches_val))\n",
        "          print(\"Seen so far: %s samples\" % (step * batch_size))\n",
        "\n"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "step: 100, loss: 0.785463, train acc: 59.375000, validation acc: 67.167102\n",
            "Seen so far: 3200 samples\n",
            "step: 200, loss: 0.315190, train acc: 87.500000, validation acc: 74.347258\n",
            "Seen so far: 6400 samples\n",
            "step: 300, loss: 0.421894, train acc: 81.250000, validation acc: 79.112272\n",
            "Seen so far: 9600 samples\n",
            "\n",
            "Start of epoch 1\n",
            "step: 100, loss: 0.407819, train acc: 71.875000, validation acc: 83.093995\n",
            "Seen so far: 3200 samples\n",
            "step: 200, loss: 0.324322, train acc: 87.500000, validation acc: 82.506527\n",
            "Seen so far: 6400 samples\n",
            "step: 300, loss: 0.530996, train acc: 78.125000, validation acc: 84.203655\n",
            "Seen so far: 9600 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKP3T5F2tHdx"
      },
      "source": [
        "### Accuracy on the whole training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVO3-qHFhAav",
        "outputId": "866ca9d7-3976-47b7-8eba-343718fd0102"
      },
      "source": [
        "train_acc = 0\n",
        "num_batches_train = 0\n",
        "for (batch_x, batch_y) in train_tf_dataset:  \n",
        "    num_batches_train += 1\n",
        "    train_prediction = tf.nn.softmax(braintumor_classification_model(batch_x))\n",
        "    train_acc += accuracy(train_prediction, batch_y)\n",
        " \n",
        "print(\"Train accuracy: \", train_acc / num_batches_train)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy:  86.96148825065274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP235pXU09hK"
      },
      "source": [
        "### Accuracy on validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-msBf04mMri",
        "outputId": "b2efb2c2-fdb7-4cdc-9b9b-d8db397e2e74"
      },
      "source": [
        "valid_acc = 0\n",
        "\n",
        "for (batch_x, batch_y) in valid_tf_dataset:  \n",
        "    valid_prediction = tf.nn.softmax(braintumor_classification_model(batch_x))\n",
        "    valid_acc += accuracy(valid_prediction, batch_y)\n",
        " \n",
        "print(\"Validation accuracy: \", valid_acc / num_batches_val)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy:  84.53002610966058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV_rnEGL1MSZ"
      },
      "source": [
        "### Accuracy on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUXGmJJlrmQf",
        "outputId": "f4c235c7-240f-44de-95ce-0099613b8673"
      },
      "source": [
        "num_batches_test = 4 \n",
        "batch_size_test = int(len(test_dataset) / num_batches_test) # 4 batches, 383 examples in each batch\n",
        "print(\"Num of examples in one batch:\", batch_size_test)\n",
        "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_dataset, test_labels)).batch(batch_size_test)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of examples in one batch: 383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzsVSV6krrY4",
        "outputId": "5f29dc65-38ed-4139-a9bf-e1c6ae03a57e"
      },
      "source": [
        "test_acc = 0\n",
        "\n",
        "for (batch_x, batch_y) in test_tf_dataset:  \n",
        "    test_prediction = tf.nn.softmax(braintumor_classification_model(batch_x))\n",
        "    test_acc += accuracy(test_prediction, batch_y)\n",
        " \n",
        "print(\"Test accuracy: \", test_acc / num_batches_test)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  86.22715404699738\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}