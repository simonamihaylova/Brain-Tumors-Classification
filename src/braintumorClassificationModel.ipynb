{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "braintumorClassificationModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18RYZdfWlkVj"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX3Ptt7iHFko"
      },
      "source": [
        "## Importing the augmented dataset:\n",
        "*   `dataset_augmentation.ipynb` - Loads the original Brain Tumor Dataset (3064 T1-Weighted MRI images) and augments the dataset using techniques such as rotating, mirroring, flipping over an axis and salting.\n",
        "*   `augmented_images.npz` - Contains the full dataset after augmentation (15320 images and 15320 labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C8aFpWdfkmX",
        "outputId": "6597946b-7c6a-4fcc-ca10-510ff5f44de4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86WAVLHWfrAu"
      },
      "source": [
        "#images_path = '../dataset/augmented_images.npz'\n",
        "images_path = '/content/gdrive/MyDrive/augmented_images.npz'"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p8k914CgCt-",
        "outputId": "e908a7d2-6207-4e69-9dbe-6dc8f05f2abf"
      },
      "source": [
        "with np.load(images_path) as data:\n",
        "    \n",
        "  images = data['images']\n",
        "  labels = data['labels']\n",
        "  print('images: ', images.shape)\n",
        "  print('labels:', labels.shape)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images:  (15320, 128, 128)\n",
            "labels: (15320,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNYgyzkuW13z"
      },
      "source": [
        "## Reformatting the data\n",
        "*   reformat into a tensorflow-friendly shape\n",
        "*   shuffle the data\n",
        "*   split the dataset into train, validation and test dataset with the following ratio: 80, 10, 10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU_fkUiOOJ9b",
        "outputId": "5e8ee655-5a67-4443-978a-cf9930cf6274"
      },
      "source": [
        "num_labels = 3\n",
        "num_channels = 1 # MRI images are grayscale\n",
        "image_size = 128\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape(\n",
        "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "\n",
        "images, labels = reformat(images, labels)\n",
        "print('images:', images.shape)\n",
        "print('labels:', labels.shape)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images: (15320, 128, 128, 1)\n",
            "labels: (15320, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkgpcY5Z8JZ"
      },
      "source": [
        "# Shufling the two numpy arrays in unison\n",
        "from sklearn import utils\n",
        "images, labels = utils.shuffle(images,labels)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5xnI3jzYqMV",
        "outputId": "39ddec13-df8d-470a-ae8d-e9c951d2495a"
      },
      "source": [
        "train_dataset, remaining_dataset, train_labels, remaining_labels = train_test_split(images, labels, train_size=0.8)\n",
        "valid_dataset,test_dataset, valid_labels, test_labels = train_test_split(remaining_dataset, remaining_labels, test_size=0.5)\n",
        "\n",
        "print('Training set:', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set:', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set:', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (12256, 128, 128, 1) (12256, 3)\n",
            "Validation set: (1532, 128, 128, 1) (1532, 3)\n",
            "Test set: (1532, 128, 128, 1) (1532, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB2DksU_sUF4",
        "outputId": "3d2741a8-a93b-480b-8ab0-35e0bdfe8a0d"
      },
      "source": [
        "print(train_labels[0], train_labels[10416], train_labels[2010])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 0.] [0. 0. 1.] [1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5AnQ1tctZxC",
        "outputId": "157ecf6f-bad0-4c11-f059-df0e20bba943"
      },
      "source": [
        "print(valid_labels[30], valid_labels[1531], valid_labels[200])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0.] [1. 0. 0.] [1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbGysfpbq1I8"
      },
      "source": [
        "## Defining the brain tumor classification model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIRZ6rKSx_NC"
      },
      "source": [
        "batch_size = 16\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_dataset, train_labels)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKglS2kIkN3d"
      },
      "source": [
        "# Architecture:\n",
        "# Input: 128 x 128 x 1\n",
        "# Conv1: 128 x 128 x 64\n",
        "# MaxPool1: 64 x 64 x 64\n",
        "# Conv2: 64 x 64 x 128\n",
        "# MaxPool2: 32 x 32 x 128\n",
        "# Conv3: 32 x 32 x 256\n",
        "# MaxPool3: 16 x 16 x 256\n",
        "# FC: 16 * 16 * 256 , 256\n",
        "# Output: 256, 3\n",
        "\n",
        "filter_size = 3\n",
        "depth_conv1 = 64\n",
        "depth_conv2 = 128\n",
        "depth_conv3 = 256\n",
        "\n",
        "weights = {\n",
        "    'wc1' : tf.Variable(tf.random.truncated_normal([filter_size, filter_size, num_channels, depth_conv1], stddev=0.1)),\n",
        "    'wc2' : tf.Variable(tf.random.truncated_normal([filter_size, filter_size, depth_conv1, depth_conv2], stddev=0.1)),\n",
        "    'wc3' : tf.Variable(tf.random.truncated_normal([filter_size, filter_size, depth_conv2, depth_conv3], stddev=0.1)),\n",
        "    'wfc' : tf.Variable(tf.random.truncated_normal([16 * 16 * depth_conv3, depth_conv3], stddev = 0.1)),\n",
        "    'wout': tf.Variable(tf.random.truncated_normal([depth_conv3, num_labels], stddev = 0.1)),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1' : tf.Variable(tf.constant(1.0, shape=[depth_conv1])),\n",
        "    'bc2' : tf.Variable(tf.constant(1.0, shape=[depth_conv2])),\n",
        "    'bc3' : tf.Variable(tf.constant(1.0, shape=[depth_conv3])),\n",
        "    'bfc' : tf.Variable(tf.constant(1.0, shape=[depth_conv3])),\n",
        "    'bout': tf.Variable(tf.constant(1.0, shape=[num_labels])),\n",
        "}\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMfZNXlb3qMO"
      },
      "source": [
        "# Wrapper functions for the convolutional and max pooling layers\n",
        "def conv2d(x, W, b, stride=1):\n",
        "  x = tf.nn.conv2d(x, W, strides=[1,stride,stride,1], padding='SAME')\n",
        "  x = tf.nn.bias_add(x, b)\n",
        "  return tf.nn.relu(x)\n",
        "\n",
        "def maxpool(x, k = 2):\n",
        "  return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,k,k,1], padding='SAME')\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ95OKGN62PL"
      },
      "source": [
        "def braintumor_classification_model(data):\n",
        "  conv_layer1 = conv2d(data, weights['wc1'], biases['bc1'])\n",
        "  conv_layer1 = maxpool(conv_layer1)\n",
        "\n",
        "  conv_layer2 = conv2d(conv_layer1, weights['wc2'], biases['bc2'])\n",
        "  conv_layer2 = maxpool(conv_layer2)\n",
        "\n",
        "  conv_layer3 = conv2d(conv_layer2, weights['wc3'], biases['bc3'])\n",
        "  conv_layer3 = maxpool(conv_layer3)\n",
        "\n",
        "  conv_layer3_shape = conv_layer3.get_shape().as_list()\n",
        "  fc_layer = tf.reshape(conv_layer3, [conv_layer3_shape[0], conv_layer3_shape[1] * conv_layer3_shape[2] * conv_layer3_shape[3]])\n",
        "  fc_layer = tf.add(tf.matmul(fc_layer, weights['wfc']), biases['bfc'])\n",
        "  fc_layer = tf.nn.relu(fc_layer)\n",
        "\n",
        "  output_layer = tf.add(tf.matmul(fc_layer, weights['wout']), biases['bout'])\n",
        "  return output_layer"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxtLhKGg-eGb"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d29cMfrd-pNO"
      },
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m0Sduke_LHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3965ac-c8c1-4d4b-c34e-492b7cb4289f"
      },
      "source": [
        "num_steps = 100\n",
        "display_step = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for step, (batch_x, batch_y) in enumerate(train_tf_dataset.take(num_steps), 1):  \n",
        "    \n",
        "  # Training computation.\n",
        "  with tf.GradientTape() as g:\n",
        "    logits = braintumor_classification_model(batch_x)\n",
        "    loss = compute_loss(batch_y, logits)\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer.minimize(loss, g.watched_variables(), tape=g)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  if step % display_step == 0:\n",
        "    train_prediction = tf.nn.softmax(logits)\n",
        "    train_acc = accuracy(train_prediction, batch_y)\n",
        "    print(\"step: %i, loss: %f, train acc: %f\" % (step, loss, train_acc))\n",
        "\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 10, loss: 225.957947, train acc: 56.250000\n",
            "step: 20, loss: 4.734693, train acc: 50.000000\n",
            "step: 30, loss: 2.129059, train acc: 68.750000\n",
            "step: 40, loss: 1.204660, train acc: 43.750000\n",
            "step: 50, loss: 1.514118, train acc: 43.750000\n",
            "step: 60, loss: 1.384868, train acc: 50.000000\n",
            "step: 70, loss: 0.705682, train acc: 68.750000\n",
            "step: 80, loss: 0.983278, train acc: 56.250000\n",
            "step: 90, loss: 0.765288, train acc: 75.000000\n",
            "step: 100, loss: 1.121211, train acc: 68.750000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP235pXU09hK"
      },
      "source": [
        "### Validate on batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy9W7k5rwl-C",
        "outputId": "94c71ce9-a040-4985-a854-97f44e629adf"
      },
      "source": [
        "num_batches = 4 \n",
        "batch_size = int(len(valid_dataset) / num_batches) # 4 batches, 383 examples in each batch\n",
        "print(\"Num of examples in one batch:\", batch_size)\n",
        "valid_tf_dataset = tf.data.Dataset.from_tensor_slices((valid_dataset, valid_labels)).batch(batch_size)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of examples in one batch: 383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-msBf04mMri",
        "outputId": "8f86d605-d1e9-42eb-c459-226600cffb38"
      },
      "source": [
        "valid_acc = 0\n",
        "\n",
        "for (batch_x, batch_y) in valid_tf_dataset:  \n",
        "    valid_prediction = tf.nn.softmax(braintumor_classification_model(batch_x))\n",
        "    valid_acc += accuracy(valid_prediction, batch_y)\n",
        " \n",
        "print(\"Validation accuracy: \", valid_acc / num_batches)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy:  59.39947780678851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV_rnEGL1MSZ"
      },
      "source": [
        "### Test on batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUXGmJJlrmQf",
        "outputId": "909c8b8e-9fe4-4663-b8c4-27c34f19c136"
      },
      "source": [
        "num_batches = 4 \n",
        "batch_size = int(len(test_dataset) / num_batches) # 4 batches, 383 examples in each batch\n",
        "print(\"Num of examples in one batch:\", batch_size)\n",
        "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_dataset, test_labels)).batch(batch_size)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of examples in one batch: 383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzsVSV6krrY4",
        "outputId": "b6f68fdb-21cd-4db7-efc0-872c54bb426e"
      },
      "source": [
        "test_acc = 0\n",
        "\n",
        "for (batch_x, batch_y) in test_tf_dataset:  \n",
        "    test_prediction = tf.nn.softmax(braintumor_classification_model(batch_x))\n",
        "    test_acc += accuracy(test_prediction, batch_y)\n",
        " \n",
        "print(\"Test accuracy: \", test_acc / num_batches)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  60.96605744125326\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}